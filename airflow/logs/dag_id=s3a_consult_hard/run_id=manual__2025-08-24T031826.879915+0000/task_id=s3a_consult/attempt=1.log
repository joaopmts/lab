{"timestamp":"2025-08-24T03:18:27.347283","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-08-24T03:18:27.347767","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/reddit_dag_s3a_consult_simple.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-08-24T03:18:27.385647","level":"warning","event":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/connection.py:471: DeprecationWarning: Using Connection.get_connection_from_secrets from `airflow.models` is deprecated.Please use `from airflow.sdk import Connection` instead\n  warnings.warn(\n","logger":"py.warnings"}
{"timestamp":"2025-08-24T03:18:27.395426","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-08-24T03:18:27.396306","level":"info","event":"Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=eDFFCzvdNdoeLlJkZhXI --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --driver-class-path /opt/airflow/conf/jars/hadoop-aws-3.3.4.jar,/opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar --jars /opt/airflow/conf/jars/hadoop-aws-3.3.4.jar,/opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar --name s3a --deploy-mode client /opt/airflow/dags/scripts/python/read_s3a.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:28.630525","level":"info","event":"25/08/24 03:18:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:28.742949","level":"info","event":"25/08/24 03:18:28 WARN DependencyUtils: Local jar /opt/airflow/conf/jars/hadoop-aws-3.3.4.jar does not exist, skipping.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:28.743400","level":"info","event":"25/08/24 03:18:28 WARN DependencyUtils: Local jar /opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar does not exist, skipping.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.782626","level":"info","event":"25/08/24 03:18:29 INFO SparkContext: Running Spark version 3.5.4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.782875","level":"info","event":"25/08/24 03:18:29 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.783161","level":"info","event":"25/08/24 03:18:29 INFO SparkContext: Java version 17.0.16","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.802255","level":"info","event":"25/08/24 03:18:29 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.802876","level":"info","event":"25/08/24 03:18:29 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.803450","level":"info","event":"25/08/24 03:18:29 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.803710","level":"info","event":"25/08/24 03:18:29 INFO SparkContext: Submitted application: s3a","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.815696","level":"info","event":"25/08/24 03:18:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.820702","level":"info","event":"25/08/24 03:18:29 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.821186","level":"info","event":"25/08/24 03:18:29 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.853267","level":"info","event":"25/08/24 03:18:29 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.853531","level":"info","event":"25/08/24 03:18:29 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.853775","level":"info","event":"25/08/24 03:18:29 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.854013","level":"info","event":"25/08/24 03:18:29 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.854301","level":"info","event":"25/08/24 03:18:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.982111","level":"info","event":"25/08/24 03:18:29 INFO Utils: Successfully started service 'sparkDriver' on port 43067.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:29.997920","level":"info","event":"25/08/24 03:18:29 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.017638","level":"info","event":"25/08/24 03:18:30 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.029698","level":"info","event":"25/08/24 03:18:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.030028","level":"info","event":"25/08/24 03:18:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.032756","level":"info","event":"25/08/24 03:18:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.047152","level":"info","event":"25/08/24 03:18:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fb6b93c1-148b-4b58-a548-9fc55b95a2ab","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.059929","level":"info","event":"25/08/24 03:18:30 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.069805","level":"info","event":"25/08/24 03:18:30 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.142330","level":"info","event":"25/08/24 03:18:30 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.187194","level":"info","event":"25/08/24 03:18:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.196439","level":"info","event":"25/08/24 03:18:30 INFO Utils: Successfully started service 'SparkUI' on port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.222770","level":"info","event":"25/08/24 03:18:30 ERROR SparkContext: Failed to add file:/opt/airflow/conf/jars/hadoop-aws-3.3.4.jar to Spark environment","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223026","level":"info","event":"java.io.FileNotFoundException: Jar /opt/airflow/conf/jars/hadoop-aws-3.3.4.jar not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223238","level":"info","event":"at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223297","level":"info","event":"at org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223326","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223349","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223371","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223392","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223412","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223432","level":"info","event":"at org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223500","level":"info","event":"at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223538","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223573","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223598","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223620","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223643","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223666","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223690","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223713","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:238)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223733","level":"info","event":"at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223753","level":"info","event":"at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223774","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223793","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.223813","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224364","level":"info","event":"25/08/24 03:18:30 ERROR SparkContext: Failed to add file:/opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar to Spark environment","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224422","level":"info","event":"java.io.FileNotFoundException: Jar /opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224451","level":"info","event":"at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224475","level":"info","event":"at org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224495","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224516","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224535","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224554","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224573","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224592","level":"info","event":"at org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224610","level":"info","event":"at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224629","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224648","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224667","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224686","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224704","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224722","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224741","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224760","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:238)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224778","level":"info","event":"at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224797","level":"info","event":"at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224815","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224832","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.224849","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.272935","level":"info","event":"25/08/24 03:18:30 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.306060","level":"info","event":"25/08/24 03:18:30 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 18 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.371228","level":"info","event":"25/08/24 03:18:30 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250824031830-0001","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.376191","level":"info","event":"25/08/24 03:18:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39391.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.376303","level":"info","event":"25/08/24 03:18:30 INFO NettyBlockTransferService: Server created on 66ff3891c557:39391","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.377493","level":"info","event":"25/08/24 03:18:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.382178","level":"info","event":"25/08/24 03:18:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 66ff3891c557, 39391, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.384669","level":"info","event":"25/08/24 03:18:30 INFO BlockManagerMasterEndpoint: Registering block manager 66ff3891c557:39391 with 434.4 MiB RAM, BlockManagerId(driver, 66ff3891c557, 39391, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.386526","level":"info","event":"25/08/24 03:18:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 66ff3891c557, 39391, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.387219","level":"info","event":"25/08/24 03:18:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 66ff3891c557, 39391, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.512095","level":"info","event":"25/08/24 03:18:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.623242","level":"info","event":"25/08/24 03:18:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:30.624748","level":"info","event":"25/08/24 03:18:30 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110559","level":"info","event":"25/08/24 03:18:31 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://teste/reddit.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110680","level":"info","event":"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110717","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110745","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110769","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110792","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110813","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110833","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110853","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110872","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110891","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110910","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110928","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110947","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110965","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.110987","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111018","level":"info","event":"at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111053","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111083","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111105","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111187","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111220","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111245","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111267","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111289","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111310","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111329","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111349","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111368","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111388","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111407","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111426","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.111446","level":"info","event":"... 26 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.152998","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.153203","level":"info","event":"File \"/opt/airflow/dags/scripts/python/read_s3a.py\", line 13, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.155451","level":"info","event":"df_hdfs = spark.read.csv(","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.155539","level":"info","event":"^^^^^^^^^^^^^^^","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.155574","level":"info","event":"File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 740, in csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.156486","level":"info","event":"File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.157638","level":"info","event":"File \"/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.158725","level":"info","event":"File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160047","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o34.csv.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160126","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160160","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160184","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160205","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160225","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160245","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160265","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160284","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160304","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160322","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160341","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160359","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160378","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160397","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160417","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160435","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160456","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160476","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160495","level":"info","event":"at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160517","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160547","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160577","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160599","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160620","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160646","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160665","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160685","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160705","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160724","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160744","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160763","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160782","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160800","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160818","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160836","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.160856","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.203857","level":"info","event":"25/08/24 03:18:31 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.204192","level":"info","event":"25/08/24 03:18:31 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.213138","level":"info","event":"25/08/24 03:18:31 INFO SparkUI: Stopped Spark web UI at http://66ff3891c557:4041","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.215647","level":"info","event":"25/08/24 03:18:31 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.217686","level":"info","event":"25/08/24 03:18:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.227456","level":"info","event":"25/08/24 03:18:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.240687","level":"info","event":"25/08/24 03:18:31 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.241003","level":"info","event":"25/08/24 03:18:31 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.245718","level":"info","event":"25/08/24 03:18:31 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.247557","level":"info","event":"25/08/24 03:18:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.257506","level":"info","event":"25/08/24 03:18:31 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.257617","level":"info","event":"25/08/24 03:18:31 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.257961","level":"info","event":"25/08/24 03:18:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3ca3fd0-9b82-4429-93ed-3390c8c72ee7/pyspark-363022e5-dfb7-428c-9d3b-54c528de1749","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.262168","level":"info","event":"25/08/24 03:18:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3ca3fd0-9b82-4429-93ed-3390c8c72ee7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.265577","level":"info","event":"25/08/24 03:18:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-93ba2663-fd9a-4b42-b0c3-a046f70e34e1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-24T03:18:31.290233","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=eDFFCzvdNdoeLlJkZhXI --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --driver-class-path /opt/airflow/conf/jars/hadoop-aws-3.3.4.jar,/opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar --jars /opt/airflow/conf/jars/hadoop-aws-3.3.4.jar,/opt/airflow/conf/jars/aws-java-sdk-bundle-1.12.262.jar --name s3a --deploy-mode client /opt/airflow/dags/scripts/python/read_s3a.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":890,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1177,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
