{"timestamp":"2025-08-24T09:57:30.451584","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-08-24T09:57:30.451937","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/reddit_silver.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-08-24T09:57:30.705157","level":"warning","event":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/connection.py:471: DeprecationWarning: Using Connection.get_connection_from_secrets from `airflow.models` is deprecated.Please use `from airflow.sdk import Connection` instead\n  warnings.warn(\n","logger":"py.warnings"}
{"timestamp":"2025-08-24T09:57:30.712052","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-08-24T09:57:31.702303Z","level":"error","event":"25/08/24 09:57:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:57:31.803991Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:57:31.804500Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:57:33.082970Z","level":"error","event":"25/08/24 09:57:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:57:50.283007Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 2]\r25/08/24 09:57:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:58:05.283787Z","level":"error","event":"25/08/24 09:58:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:58:20.282772Z","level":"error","event":"25/08/24 09:58:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:58:35.708706Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 2]\r\r[Stage 0:>                                                          (0 + 2) / 2]\r\r                                                                                \r25/08/24 09:58:35 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:58:37.729755Z","level":"error","event":"\r[Stage 1:>                                                          (0 + 4) / 4]\r\r                                                                                \r25/08/24 09:58:37 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-24T09:58:39.264417","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
